{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vqCQWoa0uUnG"
      },
      "outputs": [],
      "source": [
        "\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "from sklearn.svm import SVR\n",
        "import re\n",
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VokVZblubc8",
        "outputId": "a952ac60-a3e2-40e7-d68b-addbbc742b2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[32.2, 33.4, 40.9, 42.9, 46.2, 51.3, 50.3, 41.1, 48.7, 15.5, 23.1, 31.6, 43.5, 54.6, 59.4, 59.7, 31.0, 36.3, 47.3, 61.7, 76.3, 76.4, 76.6, 79.8, 17.0, 30.6, 52.2, 61.6, 61.3, 62.6, 64.7, 65.0, 6.5, 13.0, 22.7, 29.7, 48.8, 59.2, 60.9, 63.0, 49.1, 6.6, 10.5, 22.8, 39.0, 50.8, 58.2, 62.1, 62.7, 65.8, 68.3, 29.2, 32.3, 45.7, 54.2, 77.1, 83.7, 80.4, 73.7, 71.7, 76.6, 29.1, 41.7, 57.3, 74.2, 74.4, 72.5, 71.1, 68.6, 64.5, 14.4, 17.2, 19.7, 23.8, 29.8, 32.0, 30.2, 30.1, 28.5, 26.3, 11.0, 24.6, 34.7, 32.2, 23.5, 38.7, 58.8, 61.9, 56.9, 11.9, 14.4, 18.7, 22.8, 38.4, 63.5, 75.1, 76.9, 74.5, 71.4, 10.5, 33.8, 61.2, 76.5, 87.4, 66.9, 55.1, 46.1, 31.8, 23.6, 34.4, 46.9, 57.6, 59.6, 55.9, 6.8, 6.5, 10.7, 13.2, 20.4, 24.7, 29.1, 33.2, 32.8, 34.1, 42.3, 45.2, 49.4, 58.3, 64.7, 69.6, 67.3, 71.0, 71.9, 69.4, 18.6, 33.2, 59.6, 69.3, 81.8, 77.0, 74.7, 75.2, 20.3, 41.2, 56.6, 67.7, 74.3, 74.2, 68.7, 67.7, 67.6, 67.9, 26.6, 36.9, 47.5, 46.0, 54.5, 61.8, 75.2, 76.7, 66.5, 63.1, 13.7, 27.6, 36.2, 61.6, 73.6, 66.9, 59.0, 58.7, 62.7, 61.6, 13.1, 26.0, 34.6, 53.9, 66.1, 75.4, 76.6, 77.4, 77.2, 73.5, 20.7, 33.3, 50.0, 63.7, 81.9, 90.9, 97.6, 94.1, 77.6, 75.6, 52.8, 54.1, 50.1, 48.2, 55.7, 61.9, 53.7, 49.0, 30.9, 43.0, 55.5, 58.8, 57.0, 60.7, 66.9, 69.5, 25.5, 29.2, 45.2, 61.0, 61.0, 61.6, 54.7, 56.8, 51.0, 15.7, 36.1, 54.2, 60.7, 81.7, 87.6, 83.0, 75.2, 68.5, 66.9, 27.3, 36.1, 41.4, 28.4, 27.2, 27.1, 25.1, 23.9, 22.2, 22.9, 16.7, 20.1, 23.9, 29.0, 30.8, 30.6, 31.1, 28.9, 27.8, 26.3, 20.4, 20.7, 35.5, 49.1, 64.0, 58.8, 62.2, 60.4, 54.6, 18.9, 26.6, 32.8, 46.8, 55.4, 62.0, 59.6, 61.4, 65.1, 63.3, 22.9, 26.9, 31.3, 32.3, 36.4, 38.0, 36.6, 37.5, 64.7, 8.6, 13.6, 17.4, 34.2, 49.8, 47.0, 57.6, 71.3, 55.1, 15.7, 18.4, 20.3, 19.4, 19.7, 24.6, 24.9, 20.2, 19.1, 17.7, 24.3, 22.4, 25.1, 23.7, 27.6, 26.5, 24.5, 25.7, 8.9, 11.7, 14.0, 19.2, 18.9, 19.1, 21.2, 24.7, 26.4, 15.2, 18.5, 24.7, 26.8, 23.5, 21.5, 20.8, 22.2, 22.2, 14.0, 19.8, 21.2, 21.6, 22.9, 24.3, 21.3, 18.2, 19.5, 17.7, 16.9, 7.4, 13.6, 17.7, 19.2, 22.8, 22.7, 22.2, 19.9, 17.3, 9.1, 19.3, 27.1, 59.8, 59.0, 48.8, 50.3, 48.0, 35.9, 31.1, 31.7, 11.4, 15.4, 16.7, 20.7, 24.9, 25.5, 25.0, 21.1, 8.3, 18.0, 22.8, 20.0, 21.9, 10.3, 13.6, 23.2, 26.0, 29.8, 28.8, 31.2, 26.8, 19.4, 19.8, 17.9, 13.0, 20.9, 22.9, 21.9, 25.2, 29.5, 26.7, 21.5, 21.2, 22.1, 18.6, 29.6, 39.0, 56.1, 64.3, 47.7, 46.9, 41.7, 40.3, 36.7, 24.5, 26.0, 24.3, 25.3, 27.9, 27.5, 28.9, 25.0, 20.6, 18.5, 23.2, 27.3, 29.5, 32.4, 34.5, 34.7, 36.4, 35.4, 33.4, 32.5, 16.4, 21.2, 27.2, 28.2, 27.7, 24.5, 25.1, 26.4, 26.6, 26.6, 23.6, 22.4, 25.6, 29.1, 30.5, 27.0, 28.8, 30.6, 23.1, 20.8, 17.5, 13.3, 17.9, 21.3, 24.4, 26.3, 25.9, 27.4, 27.9, 25.1, 21.8, 24.3, 21.5, 8.9, 17.2, 21.1, 23.5, 27.2, 26.7, 28.7, 29.9, 25.7, 9.0, 16.3, 21.0, 23.5, 24.4, 28.2, 29.4, 27.3, 28.2, 23.6, 21.9, 17.9, 8.2, 14.5, 22.7, 26.7, 26.1, 25.1, 26.7, 20.1, 23.4, 19.2, 21.6, 21.2, 12.5, 18.2, 22.0, 21.1, 19.7, 21.7, 22.4, 27.3, 24.3, 18.4, 13.7, 21.0, 16.9, 17.4, 19.9, 20.8, 20.5, 21.1, 19.6, 8.5, 11.9, 18.7, 20.5, 24.4, 24.9, 23.0, 17.7, 19.6, 22.2, 24.2, 21.6, 11.5, 20.2, 22.9, 29.3, 21.5, 24.3, 23.8, 23.1, 21.9, 7.5, 12.1, 19.2, 27.2, 31.8, 30.7, 24.8, 19.8, 19.3, 20.7, 21.3, 20.8, 12.2, 18.1, 22.8, 23.0, 26.3, 25.9, 21.3, 22.7, 19.8, 20.0, 10.8, 14.9, 19.7, 21.2, 19.2, 21.0, 18.4, 19.0, 20.1, 15.9, 11.6, 18.4, 21.1, 19.2, 22.3, 21.8, 21.8, 22.6, 23.6, 21.3, 19.9, 13.2, 22.0, 26.8, 28.5, 26.7, 25.9, 20.3, 22.7, 21.8, 11.8, 14.7, 19.2, 20.6, 20.3, 21.8, 20.2, 21.3, 18.5, 17.5, 21.9, 30.0, 35.5, 37.9, 46.2, 46.9, 47.4, 46.1, 48.3, 24.9, 31.1, 30.7, 34.7, 34.7, 35.9, 37.6, 31.1, 30.3, 6.4, 12.3, 21.4, 35.0, 36.5, 38.0, 35.7, 34.3, 34.0, 29.8, 15.6, 35.9, 41.6, 64.3, 79.9, 74.0, 59.3, 87.0, 94.1, 95.5, 38.3, 53.6, 59.9, 65.2, 61.6, 62.2, 76.2, 89.2, 85.4, 82.8, 38.2, 44.3, 59.0, 74.8, 77.5, 79.4, 84.5, 82.8, 82.5, 78.6, 102.1, 114.2, 100.6, 70.6, 65.2, 61.9, 65.0, 55.1, 48.7, 42.0, 53.6, 74.8, 69.3, 85.3, 78.6, 47.0, 9.3, 15.4, 25.3, 24.9, 34.1, 33.6, 27.0, 31.5, 30.0, 20.4, 34.4, 58.1, 55.5, 60.1, 66.2, 70.2, 73.1, 72.4, 18.7, 42.7, 61.6, 74.2, 70.5, 51.3, 46.5, 49.6, 21.2, 36.5, 56.5, 69.0, 64.5, 58.8, 55.5, 61.7, 60.0, 46.3, 18.6, 35.5, 59.0, 73.7, 78.1, 87.9, 83.2, 77.4, 66.7, 54.7, 23.2, 36.6, 57.0, 72.8, 97.6, 100.3, 80.8, 82.0, 79.8, 30.1, 44.6, 71.8, 82.0, 28.6, 20.5, 18.1, 38.4, 19.5, 29.2, 47.0, 56.4, 75.9, 74.2, 76.9, 77.4, 62.3, 58.6, 22.9, 32.9, 43.6, 60.4, 65.1, 64.0, 71.3, 52.3, 52.5, 30.1, 19.0, 26.9, 34.1, 67.8, 94.4, 98.4, 88.8, 77.4, 76.1, 23.3, 35.5, 49.6, 55.0, 61.5, 66.8, 67.0, 69.2, 69.6, 73.1, 27.6, 41.5, 59.2, 80.1, 89.1, 96.2, 85.9, 70.3, 63.7, 30.1, 37.8, 62.9, 79.8, 96.2, 90.4, 79.5, 72.3, 68.2, 63.9, 18.9, 32.6, 56.1, 69.7, 81.0, 82.2, 87.9, 86.9, 79.9, 79.5, 23.4, 44.9, 64.8, 73.0, 75.3, 76.1, 78.5, 81.3, 19.7, 42.6, 57.3, 73.6, 88.3, 92.9, 62.7, 53.1, 51.2, 50.0, 19.0, 31.0, 49.5, 61.4, 81.9, 80.0, 71.8, 60.5, 18.9, 40.1, 67.3, 74.3, 80.7, 93.4, 91.1, 63.3, 59.8, 51.4, 26.0, 45.9, 56.8, 80.4, 97.2, 88.2, 91.5, 83.6, 74.2, 65.2, 13.3, 28.1, 46.3, 66.6, 80.3, 81.7, 13.4, 22.9, 65.7, 21.6, 23.8, 28.1, 35.1, 48.3, 59.3, 60.4, 56.3, 14.5, 18.1, 22.5, 34.1, 49.6, 56.0, 53.2, 53.2, 12.4, 22.0, 39.0, 56.9, 62.7, 69.4, 65.0, 62.4, 23.8, 29.8, 39.4, 57.3, 64.7, 70.4, 61.8, 13.1, 27.0, 56.1, 55.1, 55.0, 52.2, 12.3, 14.9, 16.3, 23.9, 36.0, 42.2, 47.9, 44.4, 7.6, 12.0, 15.5, 23.4, 27.6, 38.0, 7.2, 10.6, 14.9, 27.9, 37.9, 41.6, 43.3, 37.0, 5.1, 10.9, 19.0, 31.5, 56.6, 69.6, 64.3, 54.1, 8.9, 14.8, 20.8, 34.8, 50.3, 64.6, 75.4, 75.8, 16.7, 28.3, 43.6, 64.2, 69.8, 78.4, 81.2, 83.5, 19.2, 28.0, 45.9, 58.9, 58.3, 63.7, 66.0, 64.4, 14.0, 19.0, 24.3, 60.2, 55.6, 53.6, 9.3, 14.7, 20.2, 29.2, 42.1, 44.1, 51.1, 51.3, 11.7, 18.4, 29.9, 45.7, 60.5, 61.4, 60.1, 61.0, 38.1, 46.8, 55.8, 50.7, 49.6, 51.7, 19.3, 23.5, 25.7, 29.1, 35.8, 44.2, 50.9, 54.0, 18.5, 26.1, 33.2, 44.8, 51.9, 55.1, 54.1, 51.8, 7.5, 8.7, 9.6, 10.5, 11.4, 11.9, 10.9, 15.5, 8.0, 9.2, 14.2, 27.9, 38.2, 44.9, 51.3, 53.8]\n",
            "[8.17, 4.91, 5.44, 3.35, 1.84, 2.56, 1.42, 0.65, 1.02, 18.36, 17.06, 19.52, 19.44, 14.42, 8.29, 7.8, 18.39, 19.17, 18.67, 18.98, 13.55, 9.95, 10.04, 7.46, 22.8, 21.05, 18.19, 16.07, 8.03, 6.15, 6.54, 9.98, 29.97, 22.78, 22.91, 24.23, 15.59, 8.88, 7.29, 11.89, 15.27, 23.77, 26.99, 24.02, 18.68, 12.55, 12.14, 5.16, 5.76, 6.88, 7.21, 13.2, 15.45, 15.14, 21.01, 18.75, 9.71, 7.18, 4.74, 4.29, 5.42, 13.17, 12.74, 17.37, 12.35, 8.41, 5.62, 3.71, 4.92, 4.38, 25.1, 26.24, 14.17, 9.67, 8.49, 7.18, 6.45, 7.62, 9.76, 8.52, 29.34, 15.84, 12.57, 14.95, 25.15, 15.0, 8.46, 8.01, 13.6, 25.05, 25.98, 24.43, 38.66, 26.08, 14.5, 8.26, 8.05, 7.71, 7.37, 46.81, 32.1, 19.48, 15.95, 12.34, 24.1, 16.09, 11.96, 10.78, 18.53, 17.72, 16.61, 10.69, 4.03, 3.54, 20.89, 22.01, 19.59, 21.23, 18.96, 16.42, 14.45, 13.82, 14.46, 13.74, 9.5, 9.84, 9.27, 8.17, 8.5, 5.88, 6.6, 6.73, 7.68, 7.61, 28.55, 23.95, 12.66, 6.87, 3.03, 3.24, 2.93, 3.07, 35.28, 26.83, 21.89, 10.28, 5.3, 4.43, 3.64, 3.89, 4.06, 5.53, 27.51, 29.57, 19.73, 24.32, 15.01, 5.99, 8.04, 8.0, 7.61, 11.44, 38.92, 35.6, 37.95, 27.25, 14.07, 10.44, 5.68, 4.1, 4.29, 4.57, 21.26, 21.76, 26.61, 16.35, 8.78, 7.23, 5.27, 5.49, 6.2, 6.99, 20.39, 24.01, 17.7, 19.0, 13.44, 9.61, 8.62, 7.7, 5.61, 5.17, 10.88, 9.08, 3.83, 2.1, 3.65, 2.14, 1.84, 1.72, 16.99, 11.41, 7.38, 4.54, 4.2, 4.56, 3.21, 8.76, 20.93, 26.87, 24.01, 14.01, 10.26, 10.69, 14.12, 8.87, 9.93, 25.74, 27.25, 27.15, 26.53, 18.67, 12.99, 8.73, 8.73, 6.9, 6.96, 20.9, 16.39, 20.13, 15.49, 11.69, 11.05, 9.36, 8.81, 8.99, 8.0, 15.66, 18.56, 13.96, 6.54, 5.85, 5.86, 3.73, 4.21, 5.74, 5.73, 13.83, 12.48, 10.06, 5.74, 4.68, 4.79, 2.97, 4.65, 7.44, 13.52, 11.92, 13.7, 10.51, 6.87, 5.65, 4.16, 3.79, 4.0, 4.16, 21.31, 19.32, 17.01, 22.98, 10.75, 5.95, 4.72, 4.95, 5.38, 21.9, 24.54, 28.73, 24.48, 25.26, 11.33, 7.84, 6.52, 8.15, 6.5, 2.36, 0.94, 2.28, 0.91, 0.05, 0.13, 0.24, 5.93, 8.1, 4.08, 2.18, 2.35, 2.06, 0.66, 0.58, 1.14, 1.18, 9.59, 5.59, 3.16, 3.24, 3.6, 4.77, 2.91, 2.98, 2.87, 6.34, 4.37, 3.95, 4.1, 2.08, 2.49, 2.83, 4.23, 2.15, 9.05, 6.85, 2.28, 1.39, 1.55, 2.36, 1.19, 1.31, 0.79, 1.33, 3.43, 10.01, 10.58, 4.12, 2.42, 3.77, 1.11, 1.31, 4.49, 7.57, 15.98, 14.88, 10.7, 7.1, 7.3, 2.35, 1.68, 1.92, 1.6, 2.03, 1.62, 13.36, 13.02, 12.74, 7.8, 6.18, 4.63, 4.38, 5.15, 27.01, 29.33, 4.32, 2.57, 1.99, 12.03, 14.04, 17.45, 13.27, 4.68, 3.41, 2.81, 2.94, 1.07, 2.06, 3.24, 9.49, 12.15, 12.62, 6.04, 5.5, 4.96, 2.59, 0.88, 1.37, 1.09, 5.82, 6.8, 8.21, 8.74, 6.33, 3.62, 3.84, 3.54, 3.4, 4.77, 6.89, 8.8, 9.15, 7.08, 6.02, 3.92, 3.41, 5.37, 8.05, 9.7, 10.91, 7.03, 7.12, 4.92, 5.64, 4.15, 3.84, 3.97, 3.61, 3.67, 10.0, 10.1, 10.51, 4.13, 4.27, 3.31, 2.11, 1.84, 1.22, 2.08, 6.08, 5.51, 4.08, 1.83, 0.67, 0.42, 0.06, 0.67, 2.59, 3.61, 2.49, 7.62, 3.01, 1.94, 1.72, 1.38, 1.06, 0.86, 0.62, 0.82, 1.09, 1.49, 2.4, 11.78, 5.29, 3.05, 1.7, 1.53, 1.39, 0.66, 0.34, 0.78, 11.23, 8.09, 3.25, 1.82, 1.08, 0.79, 0.69, 0.54, 0.38, 2.97, 2.31, 1.72, 11.58, 11.62, 4.17, 1.86, 1.58, 0.88, 0.9, 0.52, 0.93, 0.74, 1.11, 1.45, 7.51, 7.42, 1.71, 1.27, 0.69, 0.34, 0.71, 0.65, 0.47, 1.86, 15.57, 4.39, 3.58, 1.2, 0.69, 1.26, 1.21, 1.38, 1.79, 8.04, 11.31, 7.06, 3.6, 1.55, 0.81, 0.63, 0.44, 0.73, 1.36, 2.22, 2.42, 11.43, 2.96, 1.85, 0.97, 0.84, 0.12, 0.02, 0.43, 0.46, 8.58, 7.03, 4.76, 4.21, 4.77, 2.57, 1.39, 0.88, 0.8, 1.23, 1.61, 1.99, 6.73, 9.55, 6.66, 1.32, 1.49, 2.27, 2.27, 3.03, 2.3, 4.12, 6.42, 4.25, 3.27, 2.9, 2.87, 1.73, 1.63, 2.43, 2.47, 4.08, 4.41, 5.75, 6.05, 4.85, 1.3, 1.19, 1.07, 1.07, 1.35, 0.78, 0.79, 12.58, 9.23, 8.59, 3.24, 1.19, 0.99, 1.26, 1.67, 1.73, 9.94, 7.79, 4.22, 4.13, 2.99, 2.2, 3.03, 2.91, 5.32, 12.47, 11.57, 9.42, 5.69, 2.86, 2.05, 2.24, 2.29, 3.07, 3.25, 19.67, 5.51, 2.81, 2.36, 1.24, 1.8, 1.97, 1.94, 2.71, 22.15, 21.27, 16.31, 6.8, 1.97, 1.56, 1.43, 1.47, 1.98, 2.32, 25.02, 21.18, 11.85, 10.46, 7.79, 4.47, 3.54, 7.39, 5.62, 5.81, 12.3, 5.85, 7.89, 7.27, 4.91, 2.81, 3.48, 3.44, 4.03, 5.0, 10.26, 9.19, 6.43, 7.08, 6.36, 5.2, 4.77, 3.59, 4.1, 4.33, 8.83, 7.34, 5.54, 6.2, 7.21, 4.79, 4.17, 2.91, 4.13, 3.56, 3.06, 6.14, 6.63, 5.06, 5.46, 5.57, 10.71, 11.01, 8.21, 4.26, 5.25, 6.88, 4.34, 3.57, 5.09, 19.21, 10.82, 7.85, 5.16, 3.81, 3.71, 4.1, 4.72, 5.35, 19.54, 16.35, 13.44, 5.46, 5.34, 4.18, 4.54, 4.64, 19.65, 14.44, 9.29, 9.92, 5.14, 3.72, 3.88, 4.24, 5.62, 4.78, 17.08, 20.48, 18.59, 8.77, 6.92, 7.39, 5.27, 4.92, 3.92, 4.92, 17.02, 19.22, 8.31, 11.14, 6.73, 7.65, 5.46, 7.87, 6.67, 19.26, 23.08, 11.85, 10.04, 4.64, 4.07, 5.59, 11.68, 16.0, 16.51, 8.7, 6.33, 8.19, 5.14, 5.13, 5.25, 4.49, 3.48, 7.23, 7.96, 9.52, 6.93, 6.32, 4.41, 4.56, 4.48, 3.99, 13.51, 12.54, 19.38, 16.66, 16.8, 10.35, 8.78, 7.04, 5.29, 6.23, 18.77, 21.91, 15.93, 6.45, 6.33, 5.39, 5.63, 5.35, 6.19, 5.63, 13.54, 14.41, 13.5, 10.02, 6.1, 6.44, 6.18, 5.79, 6.31, 17.41, 17.57, 10.48, 15.35, 10.82, 8.87, 9.1, 6.8, 5.72, 7.33, 14.75, 13.12, 10.12, 14.9, 15.05, 8.33, 8.12, 7.42, 7.99, 8.96, 20.21, 15.14, 9.36, 6.34, 5.64, 5.25, 5.04, 5.64, 14.41, 10.89, 12.73, 12.98, 7.32, 6.91, 4.0, 2.88, 3.94, 7.05, 15.81, 17.0, 14.75, 21.29, 12.72, 9.2, 6.41, 8.99, 14.2, 15.54, 17.54, 13.13, 8.82, 8.58, 7.81, 6.77, 5.57, 8.27, 19.82, 16.2, 21.64, 19.17, 9.35, 7.11, 8.45, 7.94, 7.51, 6.89, 20.6, 22.0, 20.6, 16.0, 11.3, 8.7, 19.4, 17.3, 7.6, 16.0, 16.5, 15.2, 15.1, 13.3, 11.9, 12.1, 13.4, 19.9, 20.4, 19.9, 17.9, 14.6, 14.2, 17.6, 17.0, 20.4, 18.4, 16.9, 13.8, 12.9, 10.5, 8.4, 8.1, 15.5, 18.4, 19.1, 12.8, 12.1, 7.6, 7.1, 17.1, 18.8, 6.5, 7.1, 7.3, 8.5, 16.5, 16.9, 17.9, 17.9, 17.3, 19.0, 16.0, 16.9, 20.0, 19.5, 23.8, 20.7, 23.6, 21.6, 13.5, 13.3, 14.0, 15.5, 16.5, 14.5, 9.8, 11.1, 17.4, 16.8, 18.9, 21.9, 18.2, 11.7, 10.6, 8.4, 15.2, 19.9, 20.7, 16.7, 12.1, 10.3, 8.6, 9.4, 18.6, 20.9, 19.9, 15.0, 12.7, 10.7, 11.9, 11.4, 26.6, 26.0, 22.3, 25.3, 12.7, 10.2, 11.6, 11.3, 15.4, 18.8, 17.1, 10.4, 10.2, 15.3, 19.8, 23.2, 24.0, 19.3, 15.2, 18.6, 15.2, 14.0, 18.3, 20.9, 22.9, 17.4, 10.5, 8.5, 9.5, 8.6, 19.0, 17.7, 15.9, 8.7, 6.4, 8.5, 12.1, 11.6, 11.7, 11.1, 10.8, 9.7, 10.0, 11.0, 12.7, 11.4, 13.0, 9.7, 9.2, 8.2, 8.9, 9.6, 21.1, 20.9, 23.2, 25.4, 27.8, 24.4, 27.8, 22.6, 18.4, 18.3, 19.1, 18.8, 21.3, 15.6, 10.4, 11.4]\n",
            "[1.6, 1.5, 2.3, 2.1, 2.8, 3.2, 3.0, 4.0, 3.5, 4.1, 2.9, 3.0, 4.2, 4.5, 5.4, 5.5, 1.2, 1.3, 2.0, 3.0, 3.1, 3.6, 5.4, 4.1, 1.4, 1.5, 2.2, 2.9, 2.0, 4.4, 3.8, 2.6, 2.6, 2.9, 2.2, 1.6, 2.7, 1.4, 2.4, 2.7, 2.9, 1.3, 1.5, 1.8, 2.7, 2.5, 2.1, 2.8, 2.2, 2.4, 2.7, 2.0, 1.4, 1.4, 2.3, 4.1, 4.4, 3.7, 4.5, 4.3, 4.1, 0.5, 1.5, 3.1, 3.4, 3.1, 2.8, 2.9, 2.7, 3.0, 3.3, 3.4, 2.2, 3.0, 3.6, 2.7, 3.7, 2.5, 3.8, 2.8, 0.9, 1.9, 3.1, 3.4, 3.7, 3.2, 3.1, 3.0, 2.2, 0.5, 1.2, 1.6, 2.6, 4.4, 3.5, 4.6, 4.3, 4.0, 3.0, 1.0, 1.3, 2.7, 3.0, 4.0, 5.0, 4.2, 3.3, 3.8, 0.7, 2.1, 2.5, 3.2, 2.9, 2.9, 1.2, 1.0, 1.6, 2.4, 3.2, 2.8, 4.1, 2.8, 2.4, 1.9, 1.4, 1.7, 2.4, 2.8, 2.9, 4.1, 5.2, 4.8, 4.9, 4.4, 0.7, 1.4, 2.7, 2.3, 3.6, 3.5, 3.7, 3.1, 0.5, 1.6, 2.4, 2.1, 2.6, 3.0, 3.9, 4.0, 3.5, 2.8, 1.1, 1.2, 2.2, 2.6, 2.8, 3.0, 2.7, 2.9, 3.0, 2.5, 1.1, 1.5, 2.1, 2.8, 3.4, 3.2, 3.3, 2.9, 3.2, 2.9, 1.4, 1.4, 2.3, 2.8, 3.1, 3.0, 3.1, 2.4, 2.5, 3.0, 3.7, 2.4, 3.0, 2.7, 4.2, 5.3, 5.0, 6.2, 5.7, 5.9, 2.1, 1.9, 1.8, 0.7, 4.2, 1.8, 3.4, 3.9, 2.0, 1.5, 2.0, 1.8, 2.2, 2.7, 2.7, 2.7, 2.2, 2.1, 3.6, 4.5, 4.2, 3.5, 4.3, 5.5, 3.1, 2.6, 2.3, 2.3, 2.5, 2.8, 2.8, 3.4, 3.7, 3.6, 3.8, 2.3, 2.4, 2.1, 3.1, 3.0, 3.1, 3.5, 3.4, 3.7, 3.1, 1.4, 1.8, 1.1, 2.6, 2.7, 2.9, 3.2, 2.1, 1.9, 1.5, 1.1, 0.9, 2.1, 3.4, 3.7, 3.6, 4.1, 4.4, 4.6, 2.0, 1.2, 2.3, 2.8, 2.9, 2.9, 3.3, 2.6, 2.8, 2.2, 2.0, 2.6, 2.7, 3.3, 4.4, 3.8, 3.7, 3.9, 4.1, 1.1, 2.1, 1.6, 2.4, 2.7, 1.6, 2.4, 2.8, 3.5, 1.4, 2.1, 1.7, 2.3, 1.9, 3.2, 3.6, 2.8, 1.6, 2.3, 2.9, 2.3, 2.3, 2.5, 2.8, 3.2, 3.2, 1.8, 0.9, 1.0, 1.9, 2.2, 2.6, 3.6, 4.0, 3.4, 3.1, 1.4, 2.8, 3.0, 3.4, 3.2, 3.8, 3.4, 3.3, 3.0, 0.7, 1.1, 0.9, 2.0, 2.3, 2.2, 2.9, 2.2, 1.9, 1.6, 1.8, 0.6, 1.4, 2.0, 1.9, 1.3, 1.7, 2.3, 0.2, 2.7, 2.3, 0.9, 0.8, 2.1, 1.9, 1.8, 2.3, 2.5, 2.6, 2.2, 1.9, 2.4, 2.4, 2.8, 2.6, 3.2, 2.9, 2.8, 2.8, 1.3, 1.8, 1.6, 0.9, 1.8, 1.3, 0.1, 1.8, 2.4, 2.8, 3.1, 2.8, 2.7, 3.1, 2.5, 2.0, 1.5, 2.5, 2.9, 2.2, 2.4, 2.7, 2.1, 2.0, 1.6, 1.6, 0.8, 1.8, 2.8, 2.4, 4.0, 4.0, 4.8, 4.7, 5.8, 4.8, 4.5, 5.8, 5.8, 6.0, 5.3, 5.7, 5.1, 5.8, 4.9, 5.0, 2.7, 3.5, 4.3, 5.3, 4.6, 5.5, 4.8, 4.6, 3.7, 3.0, 0.6, 1.0, 1.7, 2.4, 2.5, 2.5, 2.6, 2.5, 3.8, 3.3, 2.1, 2.7, 2.9, 3.5, 3.9, 4.4, 4.5, 5.2, 5.2, 5.1, 4.6, 1.7, 1.6, 2.2, 2.7, 3.2, 3.9, 3.9, 3.9, 3.3, 4.3, 4.3, 3.8, 0.8, 1.7, 2.0, 2.6, 3.7, 3.5, 4.0, 4.1, 3.2, 1.1, 0.7, 1.7, 1.8, 3.0, 3.3, 3.4, 4.0, 4.8, 4.8, 5.2, 3.6, 2.0, 0.3, 1.9, 2.6, 3.2, 3.3, 4.1, 3.9, 4.4, 4.3, 3.4, 2.9, 1.0, 1.3, 0.9, 2.5, 3.3, 3.7, 3.5, 3.5, 3.7, 2.1, 1.9, 2.0, 2.2, 4.0, 4.3, 4.1, 3.7, 4.3, 4.0, 1.6, 1.3, 2.4, 2.9, 3.3, 3.0, 3.7, 4.1, 4.3, 3.7, 3.1, 2.7, 0.3, 1.6, 2.8, 2.6, 3.5, 3.6, 3.2, 3.0, 3.2, 1.2, 1.3, 1.5, 1.4, 1.8, 2.8, 2.4, 2.8, 2.2, 2.1, 2.1, 2.5, 0.6, 1.5, 2.0, 2.2, 2.3, 2.2, 1.6, 2.4, 3.3, 2.4, 4.0, 3.5, 3.9, 3.5, 4.3, 3.6, 3.7, 3.8, 2.8, 3.1, 3.1, 2.2, 2.9, 2.5, 3.0, 2.8, 2.2, 2.7, 2.9, 2.0, 1.7, 1.1, 0.6, 1.4, 2.1, 2.0, 2.4, 2.6, 2.2, 2.5, 2.1, 4.1, 3.9, 2.7, 1.8, 3.5, 1.6, 1.2, 2.3, 2.7, 3.7, 4.8, 5.9, 6.4, 6.6, 6.3, 7.6, 5.9, 4.9, 0.3, 2.8, 3.0, 3.2, 2.3, 3.5, 2.4, 2.3, 2.3, 1.9, 0.4, 1.5, 2.0, 2.2, 2.8, 2.8, 2.0, 2.5, 1.8, 1.6, 0.5, 2.1, 1.9, 3.0, 4.0, 3.4, 3.4, 2.5, 1.7, 3.1, 3.6, 2.8, 3.8, 4.4, 6.3, 5.4, 5.3, 5.7, 5.6, 2.1, 1.6, 1.7, 1.9, 2.2, 2.8, 2.4, 1.6, 1.4, 2.0, 3.0, 3.4, 2.9, 2.0, 2.1, 3.8, 3.4, 2.8, 2.5, 2.4, 2.5, 1.9, 1.9, 1.5, 1.8, 1.3, 1.7, 0.4, 1.5, 2.0, 2.6, 3.0, 2.9, 3.5, 2.6, 1.1, 1.4, 3.3, 3.0, 3.5, 5.2, 4.3, 3.3, 2.3, 0.9, 1.1, 2.4, 3.5, 3.4, 3.6, 3.2, 3.0, 0.8, 1.1, 2.6, 3.9, 3.9, 4.0, 4.2, 3.8, 3.0, 2.7, 0.7, 1.6, 1.4, 3.1, 4.0, 4.4, 3.5, 3.7, 2.8, 2.7, 0.1, 2.0, 2.8, 3.5, 4.5, 4.9, 5.0, 4.9, 4.3, 2.1, 1.9, 1.9, 3.1, 3.2, 3.6, 3.6, 3.2, 0.8, 1.5, 3.1, 3.7, 3.7, 5.0, 5.2, 3.5, 3.2, 2.1, 0.7, 1.0, 1.5, 2.1, 1.8, 2.2, 2.4, 2.2, 2.4, 1.7, 0.3, 2.9, 2.8, 3.0, 2.9, 3.1, 3.0, 3.0, 2.9, 0.8, 1.8, 2.2, 2.9, 3.5, 3.7, 4.1, 4.2, 3.1, 3.0, 1.1, 1.5, 2.5, 1.4, 3.3, 3.2, 3.0, 3.4, 3.5, 3.2, 1.8, 1.2, 2.7, 2.3, 2.8, 2.9, 2.7, 2.8, 1.9, 2.0, 1.8, 2.3, 2.0, 2.2, 2.8, 3.1, 2.3, 3.2, 1.9, 0.6, 1.9, 2.4, 2.6, 3.4, 3.4, 3.8, 3.7, 2.2, 1.9, 2.5, 2.4, 3.7, 3.6, 4.5, 4.4, 4.2, 2.7, 2.4, 1.0, 1.5, 2.0, 2.8, 2.7, 3.4, 3.3, 0.4, 2.3, 2.5, 2.9, 3.0, 3.2, 2.9, 3.3, 2.5, 1.6, 2.0, 2.1, 4.1, 3.9, 2.7, 1.8, 3.5, 1.6, 1.2, 2.3, 0.5, 1.4, 1.6, 2.1, 3.7, 4.1, 2.4, 3.2, 3.5, 3.0, 2.7, 1.8, 2.6, 1.9, 1.6, 2.2, 3.8, 2.6, 2.6, 1.7, 1.2, 2.5, 3.5, 3.4, 3.3, 1.9, 2.1, 2.5, 3.4, 3.1, 3.6, 3.0, 3.3, 2.4, 2.7, 3.0, 2.0, 2.8, 2.8, 2.7, 3.2, 2.4, 4.6, 4.4, 4.0, 2.5, 3.1, 1.9, 2.6, 2.4, 3.2, 3.4, 2.2, 3.4, 1.5, 1.3, 2.6, 1.1, 0.2, 0.6, 2.5, 1.5, 2.4, 0.2, 3.2, 3.5, 2.6, 2.5, 1.8, 0.8, 1.8, 3.1, 2.9, 2.4, 3.3, 4.8, 1.3, 2.5, 4.1, 2.7, 1.9, 2.3, 1.6, 2.9, 0.5, 2.4, 4.1, 3.4, 0.8, 2.3, 3.4, 2.5, 1.5, 2.4, 2.3, 4.7, 5.0, 4.5, 5.2, 3.8, 3.0, 3.8, 2.7, 3.7, 4.0, 1.7, 2.0, 2.8, 3.4, 2.3, 3.1, 2.6, 2.9, 1.6, 3.3, 1.6, 3.7, 3.6, 3.7, 3.3, 3.2, 3.1, 3.8, 4.3, 3.8, 4.2, 4.8, 3.7, 4.3, 3.2, 4.4, 3.5, 2.5, 2.3, 2.6, 2.5, 2.6, 2.8, 2.1, 2.9, 4.3, 3.2, 3.1, 3.1, 1.6, 0.3, 1.2, 0.3, 1.4, 2.0, 2.2, 1.7, 2.7, 1.6, 1.1, 0.6, 0.5, 1.7, 2.4, 1.9]\n",
            "[2.0, 4.0, 5.0, 6.0, 7.0, 7.0, 5.0, 2.0, 1.0, 2.0, 3.0, 5.0, 6.0, 6.0, 5.0, 3.0, 1.0, 3.0, 5.0, 7.0, 7.0, 6.0, 4.0, 1.0, 1.0, 2.0, 5.0, 6.0, 7.0, 6.0, 4.0, 1.0, 2.0, 4.0, 5.0, 3.0, 3.0, 7.0, 5.0, 2.0, 1.0, 1.0, 2.0, 4.0, 7.0, 9.0, 9.0, 8.0, 6.0, 3.0, 1.0, 1.0, 2.0, 4.0, 6.0, 7.0, 8.0, 7.0, 4.0, 2.0, 1.0, 2.0, 5.0, 7.0, 8.0, 7.0, 6.0, 5.0, 2.0, 1.0, 1.0, 2.0, 4.0, 7.0, 9.0, 9.0, 8.0, 5.0, 3.0, 1.0, 2.0, 3.0, 5.0, 7.0, 8.0, 7.0, 4.0, 2.0, 1.0, 1.0, 2.0, 4.0, 6.0, 8.0, 8.0, 7.0, 5.0, 3.0, 1.0, 1.0, 2.0, 6.0, 7.0, 7.0, 6.0, 4.0, 2.0, 1.0, 2.0, 4.0, 7.0, 9.0, 9.0, 8.0, 1.0, 2.0, 4.0, 7.0, 9.0, 10.0, 8.0, 6.0, 3.0, 1.0, 1.0, 2.0, 5.0, 7.0, 9.0, 10.0, 9.0, 6.0, 3.0, 1.0, 1.0, 3.0, 6.0, 8.0, 9.0, 6.0, 3.0, 1.0, 1.0, 2.0, 5.0, 7.0, 10.0, 9.0, 8.0, 5.0, 3.0, 1.0, 1.0, 2.0, 4.0, 8.0, 9.0, 9.0, 8.0, 6.0, 3.0, 1.0, 1.0, 2.0, 4.0, 6.0, 7.0, 8.0, 6.0, 5.0, 2.0, 1.0, 1.0, 2.0, 4.0, 6.0, 8.0, 8.0, 7.0, 5.0, 2.0, 1.0, 1.0, 2.0, 4.0, 6.0, 8.0, 8.0, 6.0, 4.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 2.0, 1.0, 1.0, 1.0, 2.0, 2.0, 5.0, 6.0, 4.0, 3.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 3.0, 4.0, 3.0, 1.0, 1.0, 2.0, 4.0, 7.0, 8.0, 8.0, 7.0, 4.0, 2.0, 1.0, 1.0, 2.0, 5.0, 7.0, 8.0, 6.0, 6.0, 4.0, 2.0, 1.0, 1.0, 2.0, 4.0, 6.0, 9.0, 9.0, 8.0, 5.0, 3.0, 1.0, 1.0, 2.0, 4.0, 9.0, 9.0, 8.0, 5.0, 3.0, 1.0, 1.0, 2.0, 4.0, 6.0, 7.0, 7.0, 6.0, 4.0, 2.0, 1.0, 1.0, 2.0, 4.0, 6.0, 7.0, 8.0, 6.0, 4.0, 2.0, 1.0, 2.0, 5.0, 7.0, 8.0, 9.0, 7.0, 3.0, 1.0, 2.0, 5.0, 8.0, 11.0, 12.0, 12.0, 7.0, 2.0, 2.0, 4.0, 8.0, 11.0, 12.0, 12.0, 10.0, 7.0, 4.0, 2.0, 1.0, 2.0, 5.0, 8.0, 11.0, 12.0, 7.0, 4.0, 2.0, 2.0, 4.0, 10.0, 12.0, 12.0, 10.0, 7.0, 4.0, 2.0, 1.0, 2.0, 4.0, 8.0, 10.0, 12.0, 5.0, 5.0, 6.0, 2.0, 1.0, 1.0, 2.0, 4.0, 6.0, 9.0, 12.0, 6.0, 3.0, 5.0, 1.0, 2.0, 4.0, 7.0, 9.0, 8.0, 10.0, 8.0, 6.0, 3.0, 1.0, 1.0, 3.0, 6.0, 6.0, 7.0, 8.0, 5.0, 1.0, 3.0, 4.0, 5.0, 2.0, 1.0, 1.0, 2.0, 4.0, 7.0, 9.0, 11.0, 11.0, 10.0, 7.0, 4.0, 2.0, 1.0, 3.0, 7.0, 10.0, 11.0, 11.0, 10.0, 7.0, 4.0, 2.0, 1.0, 3.0, 6.0, 8.0, 8.0, 11.0, 10.0, 7.0, 4.0, 2.0, 2.0, 4.0, 7.0, 10.0, 12.0, 12.0, 10.0, 4.0, 3.0, 1.0, 2.0, 4.0, 7.0, 10.0, 9.0, 10.0, 9.0, 7.0, 4.0, 2.0, 1.0, 2.0, 4.0, 8.0, 10.0, 12.0, 12.0, 10.0, 4.0, 2.0, 2.0, 4.0, 8.0, 11.0, 12.0, 12.0, 10.0, 7.0, 5.0, 2.0, 1.0, 1.0, 2.0, 5.0, 9.0, 12.0, 14.0, 14.0, 12.0, 9.0, 5.0, 2.0, 1.0, 1.0, 2.0, 5.0, 9.0, 12.0, 14.0, 14.0, 12.0, 1.0, 1.0, 2.0, 5.0, 9.0, 11.0, 12.0, 10.0, 9.0, 8.0, 5.0, 2.0, 1.0, 1.0, 2.0, 3.0, 9.0, 12.0, 14.0, 14.0, 12.0, 9.0, 5.0, 2.0, 1.0, 1.0, 2.0, 5.0, 6.0, 8.0, 11.0, 12.0, 10.0, 9.0, 3.0, 2.0, 4.0, 6.0, 15.0, 13.0, 9.0, 5.0, 2.0, 1.0, 1.0, 2.0, 4.0, 8.0, 12.0, 14.0, 14.0, 11.0, 9.0, 5.0, 2.0, 1.0, 2.0, 4.0, 8.0, 12.0, 14.0, 14.0, 9.0, 6.0, 2.0, 1.0, 2.0, 5.0, 9.0, 11.0, 13.0, 13.0, 11.0, 8.0, 5.0, 2.0, 1.0, 2.0, 4.0, 7.0, 9.0, 10.0, 8.0, 3.0, 4.0, 4.0, 1.0, 2.0, 5.0, 9.0, 12.0, 14.0, 14.0, 12.0, 9.0, 5.0, 2.0, 2.0, 5.0, 8.0, 12.0, 14.0, 14.0, 12.0, 9.0, 5.0, 2.0, 1.0, 7.0, 7.0, 11.0, 13.0, 11.0, 9.0, 5.0, 2.0, 1.0, 4.0, 8.0, 12.0, 13.0, 11.0, 12.0, 4.0, 4.0, 2.0, 1.0, 4.0, 8.0, 11.0, 11.0, 12.0, 11.0, 8.0, 4.0, 1.0, 1.0, 4.0, 9.0, 10.0, 12.0, 10.0, 7.0, 4.0, 1.0, 1.0, 3.0, 7.0, 9.0, 12.0, 12.0, 10.0, 7.0, 3.0, 1.0, 1.0, 4.0, 7.0, 9.0, 11.0, 9.0, 8.0, 6.0, 3.0, 1.0, 1.0, 3.0, 6.0, 8.0, 11.0, 11.0, 9.0, 6.0, 3.0, 1.0, 1.0, 3.0, 6.0, 9.0, 11.0, 11.0, 9.0, 6.0, 3.0, 1.0, 11.0, 10.0, 6.0, 3.0, 1.0, 12.0, 9.0, 6.0, 3.0, 1.0, 11.0, 2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 4.0, 7.0, 9.0, 11.0, 11.0, 9.0, 6.0, 1.0, 1.0, 4.0, 7.0, 9.0, 11.0, 11.0, 8.0, 5.0, 1.0, 1.0, 3.0, 6.0, 10.0, 9.0, 5.0, 3.0, 1.0, 1.0, 4.0, 7.0, 9.0, 11.0, 10.0, 8.0, 4.0, 2.0, 1.0, 1.0, 3.0, 6.0, 7.0, 9.0, 8.0, 3.0, 4.0, 3.0, 1.0, 1.0, 3.0, 6.0, 9.0, 11.0, 12.0, 10.0, 7.0, 2.0, 1.0, 3.0, 7.0, 8.0, 9.0, 6.0, 3.0, 1.0, 1.0, 4.0, 7.0, 8.0, 10.0, 10.0, 8.0, 5.0, 1.0, 1.0, 1.0, 4.0, 7.0, 9.0, 11.0, 10.0, 8.0, 6.0, 3.0, 1.0, 1.0, 4.0, 7.0, 10.0, 11.0, 11.0, 9.0, 5.0, 2.0, 1.0, 3.0, 6.0, 9.0, 10.0, 10.0, 8.0, 5.0, 2.0, 1.0, 1.0, 1.0, 4.0, 6.0, 9.0, 6.0, 5.0, 3.0, 1.0, 1.0, 3.0, 5.0, 8.0, 9.0, 10.0, 8.0, 5.0, 2.0, 1.0, 1.0, 4.0, 6.0, 9.0, 12.0, 11.0, 9.0, 6.0, 3.0, 1.0, 1.0, 2.0, 5.0, 8.0, 11.0, 10.0, 8.0, 5.0, 1.0, 3.0, 6.0, 9.0, 10.0, 11.0, 9.0, 6.0, 3.0, 1.0, 1.0, 3.0, 6.0, 8.0, 10.0, 10.0, 8.0, 1.0, 1.0, 3.0, 6.0, 8.0, 8.0, 8.0, 7.0, 4.0, 2.0, 1.0, 2.0, 4.0, 8.0, 12.0, 13.0, 11.0, 12.0, 4.0, 4.0, 2.0, 1.0, 3.0, 5.0, 6.0, 6.0, 4.0, 1.0, 3.0, 1.0, 1.0, 3.0, 4.0, 4.0, 4.0, 3.0, 2.0, 1.0, 1.0, 2.0, 3.0, 4.0, 3.0, 3.0, 2.0, 1.0, 1.0, 3.0, 4.0, 5.0, 5.0, 4.0, 2.0, 1.0, 3.0, 4.0, 4.0, 4.0, 3.0, 2.0, 1.0, 1.0, 3.0, 5.0, 4.0, 2.0, 1.0, 1.0, 2.0, 3.0, 4.0, 5.0, 4.0, 2.0, 1.0, 1.0, 2.0, 3.0, 3.0, 2.0, 2.0, 1.0, 3.0, 4.0, 5.0, 5.0, 4.0, 2.0, 1.0, 1.0, 3.0, 4.0, 6.0, 5.0, 4.0, 2.0, 1.0, 1.0, 3.0, 4.0, 5.0, 6.0, 5.0, 2.0, 1.0, 1.0, 3.0, 4.0, 5.0, 5.0, 4.0, 2.0, 1.0, 1.0, 3.0, 4.0, 5.0, 5.0, 4.0, 2.0, 1.0, 1.0, 3.0, 4.0, 4.0, 2.0, 1.0, 1.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 1.0, 1.0, 3.0, 5.0, 6.0, 6.0, 4.0, 2.0, 1.0, 5.0, 6.0, 6.0, 4.0, 3.0, 1.0, 1.0, 2.0, 3.0, 4.0, 5.0, 4.0, 2.0, 1.0, 1.0, 3.0, 5.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.0, 1.0, 2.0, 3.0, 3.0, 2.0, 1.0, 1.0, 1.0, 2.0, 4.0, 5.0, 5.0, 4.0, 2.0, 1.0]\n"
          ]
        }
      ],
      "source": [
        "with open('/content/drive/MyDrive/info.csv', newline='', encoding='cp950',errors='ignore') as csvfile: #讀取資料\n",
        "  reader = csv.reader(csvfile)\n",
        "  file=[] \n",
        "  for i in reader:\n",
        "    file.append(i)\n",
        "O=[]\n",
        "N=[]\n",
        "P=[]\n",
        "W=[]\n",
        "for i in range(1,1000):\n",
        "  if float(file[i][0])!=0 and float(file[i][1])!=0 and float(file[i][2])!=0 and float(file[i][3])!=0:\n",
        "    if len(file[i][0]) != 0:\n",
        "      O.append(float(file[i][0]))\n",
        "    if len(file[i][1]) != 0:\n",
        "      N.append(float(file[i][1]))\n",
        "    if len(file[i][2]) != 0:\n",
        "      P.append(float(file[i][2]))\n",
        "    if len(file[i][3]) != 0:\n",
        "      W.append(float(file[i][3]))\n",
        "print(O)\n",
        "print(N)\n",
        "print(P)\n",
        "print(W)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iivUiAixf6DC"
      },
      "outputs": [],
      "source": [
        "#解析解線性迴歸\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        " \n",
        "\n",
        "aN=np.array(N).reshape(-1,1) #轉為行陣列\n",
        "aP=np.array(P).reshape(-1,1) #轉為行陣列\n",
        "aW=np.array(W).reshape(-1,1) #轉為行陣列\n",
        "X=np.hstack((aN,aP,aW)) #把三組輸入資料併在一起\n",
        "y=np.array(O).reshape(-1,1) #轉為行陣列\n",
        "\n",
        "slr = LinearRegression()\n",
        "slr.fit(X, y)\n",
        "y_pred = slr.predict(X)\n",
        "\n",
        "plt.scatter(aP, y, c='steelblue', edgecolor='white', s=70)\n",
        "plt.scatter(aP, slr.predict(X), color='black', lw=2) \n",
        "plt.xlabel('P')\n",
        "plt.ylabel('O')\n",
        "print(slr.coef_)\n",
        "print(slr.intercept_)\n",
        "#plt.savefig('images/10_07.png', dpi=300)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "T89T_CWTma6U"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dQEp-4GCaotH"
      },
      "outputs": [],
      "source": [
        "#梯度下降線性迴歸\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "aN=np.array(N).reshape(-1,1) #轉為行陣列\n",
        "aP=np.array(P).reshape(-1,1) #轉為行陣列\n",
        "aW=np.array(W).reshape(-1,1) #轉為行陣列\n",
        "X=np.hstack((aN,aP,aW)) #把三組輸入資料併在一起\n",
        "Y=np.array(O).reshape(-1,1) #轉為行陣列\n",
        "\n",
        "m = len(aN)  # Number of samples in data\n",
        "itersteps = 3000  # Total number of iterations\n",
        " \n",
        "theta_0 = np.zeros(itersteps) # intercept / theta_0\n",
        "theta_1 = np.zeros(itersteps) # slope / theta_1\n",
        "theta_2 = np.zeros(itersteps) # slope / theta_2\n",
        "theta_3 = np.zeros(itersteps) # slope / theta_3\n",
        "\n",
        "J = np.zeros(itersteps) # Cost function\n",
        "dJ_theta_0 = np.zeros(itersteps) # the derivative of J over theta_0\n",
        "dJ_theta_1 = np.zeros(itersteps) # the derivative of J over theta_1\n",
        "dJ_theta_2 = np.zeros(itersteps) # the derivative of J over theta_2\n",
        "dJ_theta_3 = np.zeros(itersteps) # the derivative of J over theta_3\n",
        " \n",
        "a = 0.01  # learning rate\n",
        "\n",
        "#initial guess for theta \n",
        "theta_0[0] = 5\n",
        "theta_1[0] = 1\n",
        "theta_2[0] = 30\n",
        "theta_3[0] = 3\n",
        "\n",
        "iter = 0  # Initial iteration counter\n",
        "# Gradient Descent implementation on the linear regression model\n",
        "while iter < itersteps - 1:\n",
        "    #print('Iteration: ', iter)\n",
        "    Y_new = theta_0[iter] + theta_1[iter] * aN + theta_2[iter] * aP + theta_3[iter] * aW\n",
        "    dJ_theta_1[iter] = 1 / m * np.sum((Y_new - Y) * aN)\n",
        "    dJ_theta_2[iter] = 1 / m * np.sum((Y_new - Y) * aP)\n",
        "    dJ_theta_3[iter] = 1 / m * np.sum((Y_new - Y) * aW)\n",
        "    dJ_theta_0[iter] = 1 / m * np.sum(Y_new - Y)\n",
        "    J[iter] = np.sum((Y_new - Y) ** 2)/(2*m) # Cost function\n",
        "    #J[iter] = np.sum(np.abs(Y_new - Y))/m # Cost function\n",
        "    theta_1[iter + 1] = theta_1[iter] - a * dJ_theta_1[iter]\n",
        "    theta_2[iter + 1] = theta_2[iter] - a * dJ_theta_2[iter]\n",
        "    theta_3[iter + 1] = theta_3[iter] - a * dJ_theta_3[iter]\n",
        "    theta_0[iter + 1] = theta_0[iter] - a * dJ_theta_0[iter]\n",
        "    iter += 1\n",
        "J[iter] = np.sum((Y_new - Y) ** 2)/(2*m) # the last one\n",
        "#J[iter] = np.sum(np.abs(Y_new - Y))/m # the last one\n",
        "\n",
        "# Plot outputs\n",
        "XX=np.linspace(1,len(J),len(J))\n",
        "plt.plot(XX, np.sqrt(J*2), color=\"blue\", linewidth=3)\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('error')\n",
        "\n",
        "print(theta_0[itersteps-1],theta_1[itersteps-1],theta_2[itersteps-1],theta_3[itersteps-1])\n",
        "plt.show() \n",
        "\n",
        "#print(np.sqrt(J[itersteps-1]*2))\n",
        "print(J[itersteps-1])\n",
        "print(np.max(Y),np.min(Y))\n",
        "\n",
        "#計算比值\n",
        "ratio=[]\n",
        "for i in range(len(N)):\n",
        "  ratio.append((theta_0[itersteps-1]+theta_1[itersteps-1]*N[i]+theta_2[itersteps-1]*P[i]+theta_3[itersteps-1]*W[i])/O[i])\n",
        "print(ratio)\n",
        "#精準度測試\n",
        "correct=0\n",
        "for i in range(len(ratio)):\n",
        "  if ratio[i] >= 0.7 and ratio[i] <= 1.3:\n",
        "    correct+=1\n",
        "precise=100*(correct/len(ratio))\n",
        "print(\"精確度為\")\n",
        "print(round(precise,4),end=\"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HSIRooBymmuy"
      },
      "outputs": [],
      "source": [
        "#RANSAC 線性迴歸\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import RANSACRegressor\n",
        "aN=np.array(N).reshape(-1,1) #轉為行陣列\n",
        "aP=np.array(P).reshape(-1,1) #轉為行陣列\n",
        "aW=np.array(W).reshape(-1,1) #轉為行陣列\n",
        "X=np.hstack((aN,aP,aW)) #把三組輸入資料併在一起\n",
        "y=np.array(O).reshape(-1,1) #轉為行陣列\n",
        "ransac = RANSACRegressor(LinearRegression(), # 使用的線性分類器\n",
        "                         max_trials=2000, # 迭代次數\n",
        "                         min_samples=800, # 隨機個數\n",
        "                         loss='absolute_error', \n",
        "                         residual_threshold=20.0, # 樣本與迴歸線間的距離絕對值\n",
        "                         random_state=0)\n",
        "\n",
        "ransac.fit(X, y)\n",
        "print(ransac.estimator_.coef_)\n",
        "print(ransac.estimator_.intercept_)\n",
        "\n",
        "\n",
        "inlier_mask = ransac.inlier_mask_\n",
        "outlier_mask = np.logical_not(inlier_mask)\n",
        "\n",
        "line_X = np.array([[1,1,1],[2,2,2],[3,3,3],[20,20,20]])\n",
        "line_y_ransac = ransac.predict(line_X)\n",
        "\n",
        "plt.scatter(X[inlier_mask][:,1], y[inlier_mask],\n",
        "            c='steelblue', edgecolor='white', \n",
        "            marker='o', label='Inliers')\n",
        "plt.scatter(X[outlier_mask][:,1], y[outlier_mask],\n",
        "            c='limegreen', edgecolor='white', \n",
        "            marker='s', label='Outliers')\n",
        "plt.plot(line_X[:,1], line_y_ransac, color='black', lw=2)   \n",
        "plt.xlabel('WIND')\n",
        "plt.ylabel('O3')\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "#計算比值\n",
        "ratio=[]\n",
        "for i in range(len(N)):\n",
        "  ratio.append((ransac.estimator_.intercept_+ransac.estimator_.coef_[0,1]*N[i]+ransac.estimator_.coef_[0,1]*P[i]+ransac.estimator_.coef_[0,2]*W[i])/O[i])\n",
        "print(ratio)\n",
        "#精準度測試\n",
        "correct=0\n",
        "for i in range(len(ratio)):\n",
        "  if ratio[i] >= 0.7 and ratio[i] <= 1.3:\n",
        "    correct+=1\n",
        "precise=100*(correct/len(ratio))\n",
        "print(\"精確度為\")\n",
        "print(round(precise,4),end=\"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "I1nRRmfbUThR"
      },
      "outputs": [],
      "source": [
        "#類神經網路\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#準備資料\n",
        "aN=np.array(N).reshape(-1,1) #轉為行陣列\n",
        "aP=np.array(P).reshape(-1,1) #轉為行陣列\n",
        "aW=np.array(W).reshape(-1,1) #轉為行陣列\n",
        "X=np.hstack((aN,aP,aW)) #把三組輸入資料併在一起\n",
        "y=np.array(O).reshape(-1,1) #轉為行陣列\n",
        "\n",
        "## 切分資料集:一部分訓練模型，一部分最後測試精準度用\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# 建立神經網路模型物件\n",
        "model = Sequential()\n",
        "# 加入 輸入層 第一隱藏層\n",
        "model.add(Dense(units=10, input_dim=3, kernel_initializer='normal', activation='selu'))\n",
        "# 加入 第二隱藏層\n",
        "model.add(Dense(units=10, kernel_initializer='normal', activation='selu'))\n",
        "# 加入 第三隱藏層 \n",
        "model.add(Dense(units=10, kernel_initializer='normal', activation='selu'))\n",
        "# 加入 輸出層\n",
        "model.add(Dense(1, kernel_initializer='normal'))\n",
        " \n",
        "# 編譯模型\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        " # 訓練模型\n",
        "history = model.fit(X_train, y_train ,validation_split=0.1, batch_size = 50, epochs = 100, verbose=0)\n",
        "\n",
        "# 畫出損失率\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'valid'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "#計算比值\n",
        "ratio=[]\n",
        "pred_y= model.predict(X_test)\n",
        "for i in range(len(X_test)):\n",
        "  ratio.append(pred_y[i]/y_test[i])\n",
        "print(ratio)\n",
        "#精準度測試\n",
        "correct=0\n",
        "for i in range(len(ratio)):\n",
        "  if ratio[i] >= 0.7 and ratio[i] <= 1.3:\n",
        "    correct+=1\n",
        "precise=100*(correct/len(ratio))\n",
        "print(\"精確度為\")\n",
        "print(round(precise,4),end=\"%\")\n",
        "print()\n",
        "print(model.summary())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "環境最終版",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}